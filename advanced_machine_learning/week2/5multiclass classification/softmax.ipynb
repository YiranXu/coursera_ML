{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89bc1915",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('./deeplearning.mplstyle')\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from IPython.display import display, Markdown, Latex\n",
    "from sklearn.datasets import make_blobs\n",
    "#%matplotlib widget\n",
    "from matplotlib.widgets import Slider\n",
    "from lab_utils_common import dlc\n",
    "from lab_utils_softmax import plt_softmax\n",
    "import logging\n",
    "logging.getLogger(\"tensorflow\").setLevel(logging.ERROR)\n",
    "tf.autograph.set_verbosity(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb419c2",
   "metadata": {},
   "source": [
    "> **Note**: Normally, in this course, the notebooks use the convention of starting counts with 0 and ending with N-1,  $\\sum_{i=0}^{N-1}$, while lectures start with 1 and end with N,  $\\sum_{i=1}^{N}$. This is because code will typically start iteration with 0 while in lecture, counting 1 to N leads to cleaner, more succinct equations. This notebook has more equations than is typical for a lab and thus  will break with the convention and will count 1 to N."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c635253",
   "metadata": {},
   "source": [
    "## Softmax Function\n",
    "In both softmax regression and neural networks with Softmax outputs, N outputs are generated and one output is selected as the predicted category. In both cases a vector $\\mathbf{z}$ is generated by a linear function which is applied to a softmax function. The softmax function converts $\\mathbf{z}$  into a probability distribution as described below. After applying softmax, each output will be between 0 and 1 and the outputs will add to 1, so that they can be interpreted as probabilities. The larger inputs  will correspond to larger output probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b6ed69",
   "metadata": {},
   "source": [
    "The softmax function can be written:\n",
    "$$a_j = \\frac{e^{z_j}}{ \\sum_{k=1}^{N}{e^{z_k} }} \\tag{1}$$\n",
    "The output $\\mathbf{a}$ is a vector of length N, so for softmax regression, you could also write:\n",
    "\\begin{align}\n",
    "\\mathbf{a}(x) =\n",
    "\\begin{bmatrix}\n",
    "P(y = 1 | \\mathbf{x}; \\mathbf{w},b) \\\\\n",
    "\\vdots \\\\\n",
    "P(y = N | \\mathbf{x}; \\mathbf{w},b)\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\frac{1}{ \\sum_{k=1}^{N}{e^{z_k} }}\n",
    "\\begin{bmatrix}\n",
    "e^{z_1} \\\\\n",
    "\\vdots \\\\\n",
    "e^{z_{N}} \\\\\n",
    "\\end{bmatrix} \\tag{2}\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e180bb97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_softmax(z):\n",
    "    ez=np.exp(z)\n",
    "    sm=ez/np.sum(ez)\n",
    "    return (sm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "37a33b16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.09, 0.24, 0.67])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_softmax([1,2,3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15f38da",
   "metadata": {},
   "source": [
    "## Tensorflow\n",
    "This lab will discuss two ways of implementing the softmax, cross-entropy loss in Tensorflow, the 'obvious' method and the 'preferred' method. The former is the most straightforward while the latter is more numerically stable.\n",
    "\n",
    "Let's start by creating a dataset to train a multiclass classification model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "955baed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make  dataset for example\n",
    "centers = [[-5, 2], [-2, -2], [1, 2], [5, -2]]\n",
    "X_train, y_train = make_blobs(n_samples=2000, centers=centers, cluster_std=1.0,random_state=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e021b185",
   "metadata": {},
   "source": [
    "## The Obvious\n",
    "The model below is implemented with the softmax as an activation in the final Dense layer.\n",
    "The loss function is separately specified in the `compile` directive. \n",
    "\n",
    "The loss function `SparseCategoricalCrossentropy`. The loss described in (3) above. In this model, the softmax takes place in the last layer. The loss function takes in the softmax output which is a vector of probabilities. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a511467c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-31 14:03:43.734688: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 2s 4ms/step - loss: 0.9006\n",
      "Epoch 2/10\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.4347\n",
      "Epoch 3/10\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.1883\n",
      "Epoch 4/10\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.1007\n",
      "Epoch 5/10\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.0686\n",
      "Epoch 6/10\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.0544\n",
      "Epoch 7/10\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.0470\n",
      "Epoch 8/10\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.0415\n",
      "Epoch 9/10\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.0375\n",
      "Epoch 10/10\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.0345\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x292321880>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential(\n",
    "    [ \n",
    "        Dense(25, activation = 'relu'),\n",
    "        Dense(15, activation = 'relu'),\n",
    "        Dense(4, activation = 'softmax')    # < softmax activation here\n",
    "    ]\n",
    ")\n",
    "model.compile(\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "    optimizer=tf.keras.optimizers.Adam(0.001),\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    X_train,y_train,\n",
    "    epochs=10\n",
    ")\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa9e12c",
   "metadata": {},
   "source": [
    "Because the softmax is integrated into the output layer, the output is a vector of probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "86c53492",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 0s 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-31 14:04:45.127081: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    }
   ],
   "source": [
    "p_nonpreferred = model.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "abb595d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[7.78e-03 6.42e-03 9.71e-01 1.43e-02]\n",
      " [9.97e-01 2.57e-03 1.37e-05 1.38e-06]]\n"
     ]
    }
   ],
   "source": [
    "print(p_nonpreferred [:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "12ecdd9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "largest value 0.9999999 smallest value 3.5595676e-12\n"
     ]
    }
   ],
   "source": [
    "print(\"largest value\", np.max(p_nonpreferred), \"smallest value\", np.min(p_nonpreferred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11dad71a",
   "metadata": {},
   "source": [
    "## Preferred\n",
    "In the preferred organization the final layer has a linear activation. For historical reasons, the outputs in this form are referred to as *logits*. The loss function has an additional argument: `from_logits = True`. This informs the loss function that the softmax operation should be included in the loss calculation. This allows for an optimized implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "86b25a6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "28/63 [============>.................] - ETA: 0s - loss: 1.0968"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-31 21:55:18.933964: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 0s 4ms/step - loss: 0.9035\n",
      "Epoch 2/10\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.4363\n",
      "Epoch 3/10\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.1888\n",
      "Epoch 4/10\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.1021\n",
      "Epoch 5/10\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.0700\n",
      "Epoch 6/10\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.0549\n",
      "Epoch 7/10\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.0465\n",
      "Epoch 8/10\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.0414\n",
      "Epoch 9/10\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.0376\n",
      "Epoch 10/10\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.0348\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x16d4c55b0>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preferred_model = Sequential(\n",
    "    [ \n",
    "        Dense(25, activation = 'relu'),\n",
    "        Dense(15, activation = 'relu'),\n",
    "        Dense(4, activation = 'linear')   #<-- Note\n",
    "    ]\n",
    ")\n",
    "preferred_model.compile(\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),  #<-- Note\n",
    "    optimizer=tf.keras.optimizers.Adam(0.001),\n",
    ")\n",
    "\n",
    "preferred_model.fit(\n",
    "    X_train,y_train,\n",
    "    epochs=10\n",
    ")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "10bd1cfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 0s 2ms/step\n",
      "two example output vectors:\n",
      " [[-0.61 -1.13  4.19 -0.19]\n",
      " [ 8.03  2.17 -3.08 -5.06]]\n",
      "largest value 14.787938 smallest value -8.373114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-31 21:56:09.899685: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    }
   ],
   "source": [
    "p_preferred = preferred_model.predict(X_train)\n",
    "print(f\"two example output vectors:\\n {p_preferred[:2]}\")\n",
    "print(\"largest value\", np.max(p_preferred), \"smallest value\", np.min(p_preferred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e513cc3",
   "metadata": {},
   "source": [
    "## Output:\n",
    "Note that the output are not probabilities.If the desired output are probabilities, the output should be be processed by a softmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "17b73eb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "two example output:\n",
      "[[8.02e-03 4.78e-03 9.75e-01 1.22e-02]\n",
      " [9.97e-01 2.84e-03 1.49e-05 2.05e-06]]\n",
      "largest value 0.9999995 smallest value 8.735362e-11\n"
     ]
    }
   ],
   "source": [
    "sm_preferred = tf.nn.softmax(p_preferred).numpy()\n",
    "print(f'two example output:\\n{sm_preferred[:2]}')\n",
    "print(\"largest value\", np.max(sm_preferred), \"smallest value\", np.min(sm_preferred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "723dc7c7",
   "metadata": {},
   "source": [
    "To select the most likely category, the softmax is not required. One can find the index of the largest output using np.argmax()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2e303a1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.61 -1.13  4.19 -0.19],category:2\n",
      "[ 8.03  2.17 -3.08 -5.06],category:0\n",
      "[ 5.7   2.32 -2.24 -4.28],category:0\n",
      "[-1.46  4.5  -0.23 -3.08],category:1\n",
      "[ 1.74 -0.61  7.12 -3.77],category:2\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(f'{p_preferred[i]},category:{np.argmax(p_preferred[i])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5427eb",
   "metadata": {},
   "source": [
    "## Numerical Stability (optional)\n",
    "This section discusses some of the methods employed to improve numerical stability. This is for the interested reader and is not at all required.\n",
    "### Softmax Numerical Stability\n",
    "The input's  to the softmax are the outputs of a linear layer $z_j = \\mathbf{w_j} \\cdot \\mathbf{x}^{(i)}+b$. These may\n",
    "be large numbers. The first step of the softmax algorithm computes $e^{z_j}$. This can result in an overflow error if the number gets too large. Try running the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9d8cd6d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e^{500} = 1.40e+217\n",
      "e^{600} = 3.77e+260\n",
      "e^{700} = 1.01e+304\n",
      "e^{800} = inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/v0/mbv34sgd2xdgkp1dv6283r180000gn/T/ipykernel_22893/1141864107.py:2: RuntimeWarning: overflow encountered in exp\n",
      "  ez = np.exp(z)\n"
     ]
    }
   ],
   "source": [
    "for z in [500,600,700,800]:\n",
    "    ez = np.exp(z)\n",
    "    zs = \"{\" + f\"{z}\" + \"}\"\n",
    "    print(f\"e^{zs} = {ez:0.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2a9dbb",
   "metadata": {},
   "source": [
    "The operation will generate an overflow if the exponent gets too large. Naturally, my_softmax() will generate the same errors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "65c6118e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/v0/mbv34sgd2xdgkp1dv6283r180000gn/T/ipykernel_22893/2441804751.py:2: RuntimeWarning: overflow encountered in exp\n",
      "  ez=np.exp(z)\n",
      "/var/folders/v0/mbv34sgd2xdgkp1dv6283r180000gn/T/ipykernel_22893/2441804751.py:3: RuntimeWarning: invalid value encountered in true_divide\n",
      "  sm=ez/np.sum(ez)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0., nan]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z_tmp = np.array([[500,600,700,800]])\n",
    "my_softmax(z_tmp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56535328",
   "metadata": {},
   "source": [
    "Numerical stability can be improved by reducing the size of the exponent. \n",
    "Recall \n",
    "$$ e^{a + b} = e^ae^b$$\n",
    "if the $b$  were the opposite sign of $a$, this would reduce the size of the exponent. Specifically, if you multiplied the softmax by a fraction:\n",
    "$$a_j = \\frac{e^{z_j}}{ \\sum_{i=1}^{N}{e^{z_i} }} \\frac{e^{-b}}{ {e^{-b}}}$$\n",
    "the exponent would be reduced and the value of the softmax would not change. If $b$ in $e^b$ were the largest value of the $z_j$'s, $max_j(\\mathbf{z})$, the exponent would be reduced to its smallest value.\n",
    "$$\\begin{align}\n",
    "a_j &= \\frac{e^{z_j}}{ \\sum_{i=1}^{N}{e^{z_i} }} \\frac{e^{-max_j(\\mathbf{z})}}{ {e^{-max_j(\\mathbf{z})}}} \\\\\n",
    "&= \\frac{e^{z_j-max_j(\\mathbf{z})}}{ \\sum_{i=1}^{N}{e^{z_i-max_j(\\mathbf{z})} }} \n",
    "\\end{align}$$\n",
    "It is customary to say $C=max_j(\\mathbf{z})$ since the equation would be correct with any constant C. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e9f9cd",
   "metadata": {},
   "source": [
    "$$\n",
    "a_j = \\frac{e^{z_j-C}}{ \\sum_{i=1}^{N}{e^{z_i-C} }} \\quad\\quad\\text{where}\\quad C=max_j(\\mathbf{z})\\tag{5}\n",
    "$$\n",
    "\n",
    "If we look at our troublesome example where $\\mathbf{z}$ contains 500,600,700,800, $C=max_j(\\mathbf{z})=800$:\n",
    "\\begin{align}\n",
    "\\mathbf{a}(x) =\n",
    "\\frac{1}{ e^{500-800} + e^{600-800} + e^{700-800} + e^{800-800}}\n",
    "\\begin{bmatrix}\n",
    "e^{500-800} \\\\\n",
    "e^{600-800} \\\\\n",
    "e^{700-800} \\\\\n",
    "e^{800-800} \\\\\n",
    "\\end{bmatrix}\n",
    "= \n",
    "\\begin{bmatrix}\n",
    "5.15e-131 \\\\\n",
    "1.38e-87 \\\\\n",
    "3.7e-44 \\\\\n",
    "1.0 \\\\\n",
    "\\end{bmatrix}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "25fb000d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_softmax_ns(z):\n",
    "    \"\"\"numerically stablility improved\"\"\"\n",
    "    bigz = np.max(z)\n",
    "    ez = np.exp(z-bigz)              # minimize exponent\n",
    "    sm = ez/np.sum(ez)\n",
    "    return(sm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e4a7d881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5.15e-131 1.38e-087 3.72e-044 1.00e+000] \n",
      " [5.15e-131 1.38e-087 3.72e-044 1.00e+000]\n"
     ]
    }
   ],
   "source": [
    "z_tmp = np.array([500.,600,700,800])\n",
    "print(tf.nn.softmax(z_tmp).numpy(), \"\\n\", my_softmax_ns(z_tmp))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ddb21b",
   "metadata": {},
   "source": [
    "Large values no longer cause an overflow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17cf90ef",
   "metadata": {},
   "source": [
    "### Cross Entropy Loss Numerical Stability\n",
    "The loss function associated with Softmax, the cross-entropy loss, is repeated here:\n",
    "\\begin{equation}\n",
    "  L(\\mathbf{a},y)=\\begin{cases}\n",
    "    -log(a_1), & \\text{if $y=1$}.\\\\\n",
    "        &\\vdots\\\\\n",
    "     -log(a_N), & \\text{if $y=N$}\n",
    "  \\end{cases} \n",
    "\\end{equation}\n",
    "\n",
    "Where y is the target category for this example and $\\mathbf{a}$ is the output of a softmax function. In particular, the values in $\\mathbf{a}$ are probabilities that sum to one.\n",
    "Let's consider a case where the target is two ($y=2$) and just look at the loss for that case. This will result in the loss being:  \n",
    "$$L(\\mathbf{a})= -log(a_2)$$\n",
    "Recall that $a_2$ is the output of the softmax function described above, so this can be written:\n",
    "$$L(\\mathbf{z})= -log\\left(\\frac{e^{z_2}}{ \\sum_{i=1}^{N}{e^{z_i} }}\\right) \\tag{6}$$\n",
    "This can be optimized. However, to make those optimizations, the softmax and the loss must be calculated together as shown in the 'preferred' Tensorflow implementation you saw above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e3ed7a",
   "metadata": {},
   "source": [
    "Starting from (6) above, the loss for the case of y=2:\n",
    "$log(\\frac{a}{b}) = log(a) - log(b)$, so (6) can be rewritten:\n",
    "$$L(\\mathbf{z})= -\\left[log(e^{z_2}) - log \\sum_{i=1}^{N}{e^{z_i} }\\right] \\tag{7}$$\n",
    "The first term can be simplified to just $z_2$:\n",
    "$$L(\\mathbf{z})= -\\left[z_2 - log( \\sum_{i=1}^{N}{e^{z_i} })\\right] =  \\underbrace{log \\sum_{i=1}^{N}{e^{z_i} }}_\\text{logsumexp()} -z_2 \\tag{8}$$\n",
    "It turns out that the $log \\sum_{i=1}^{N}{e^{z_i} }$ term in the above equation is so often used, many libraries have an implementation. In Tensorflow this is tf.math.reduce_logsumexp(). An issue with this sum is that the exponent in the sum could overflow if $z_i$ is large. To fix this, we might like to subtract $e^{max_j(\\mathbf{z})}$ as we did above, but this will require a bit of work:\n",
    "$$\n",
    "\\begin{align}\n",
    "   log \\sum_{i=1}^{N}{e^{z_i} } &= log \\sum_{i=1}^{N}{e^{(z_i - max_j(\\mathbf{z}) + max_j(\\mathbf{z}))}} \\tag{9}\\\\\n",
    "                          &= log \\sum_{i=1}^{N}{e^{(z_i - max_j(\\mathbf{z}))} e^{max_j(\\mathbf{z})}} \\\\\n",
    "                          &= log(e^{max_j(\\mathbf{z})}) + log \\sum_{i=1}^{N}{e^{(z_i - max_j(\\mathbf{z}))}} \\\\\n",
    "                          &= max_j(\\mathbf{z})  + log \\sum_{i=1}^{N}{e^{(z_i - max_j(\\mathbf{z}))}}\n",
    "\\end{align}\n",
    "$$\n",
    "Now, the exponential is less likely to overflow. It is customary to say $C=max_j(\\mathbf{z})$ since the equation would be correct with any constant C. We can now write the loss equation:\n",
    "    \n",
    "$$L(\\mathbf{z})= C+ log( \\sum_{i=1}^{N}{e^{z_i-C} }) -z_2  \\;\\;\\;\\text{where } C=max_j(\\mathbf{z}) \\tag{10} $$\n",
    "A computationally simpler, more stable version of the loss. The above is for an example where the target, y=2 but generalizes to any target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3c602b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf26",
   "language": "python",
   "name": "tf26"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
